## Using Python to scrape website data

We can make story with data. It can be the prime element of an investigation, and it can lead to new insights and new ways of thinking. 

Unfortunately, the *data* we mostly want is not always available in the required form always. It is not always available in packaged form and 
easily and directly downloaded. In such cases, a technique used to programmatically gather the data for us called **web scraping** can be used.

In this session, we'll go through how to use *Python* to perform this task. In this process, we will also see some basics of how web pages work 
and how data is structured. 

In the end we will also look into a new type of data scraping by using an extension for Google's Chrome web browser.

### Introduction

Some basic terminology that we need to understand before moving forward: 

* **HTML tables** - An HTML table is divided into rows (with the `<tr>` tag), and each row is divided into data cells (with the `<td>` tag). td stands for "table data," and holds the content of a data cell. A `<td>` tag can contain text, links, images, lists, forms, and other tables. 
  **Note:** HTML tables are structured just like tables in excel and by using python we can easily scrape data from tables found on a website and save the data in an excel file on a local drive. 

* **Python Library** - A library is a collection of standard programs and subroutines that are stored and available for immediate use.

* **Browser Extensions** - A computer program that extends the functionality of a web browser in some way.

### Setting up

We will use the following libraries to perform our *data scraping*:

* **Beautiful Soup** - A library designed for screen-scraping HTML and XML in Python.
* **lxml** - A library for processing XML and HTML in Python. 

To install these libraries using the **MAC OSX** operating system, open terminal and type in the following commands, one at a time:

~~~{.python}
sudo easy_install pip 
pip install BeautifulSoup4
pip install lxml
~~~

**Windows 7 & 8 users** Open the command prompt and navigate to your root `C:/` directory and type in the following commands, one at a time: 

~~~{.python}
easy_install BeautifulSoup4
easy_install lxml 
~~~

Our libraries are now installed and it is time to start writing our data scraping code.

### Scraping in Python

Before we begin scraping we need an objective. In this session, we will try to scrape the current [Data.gov](http://www.data.gov/) US Government website that aims to improve public access. It contains 
machine readable datasets generated by the Executive Branch of the Federal Government.

Initially we need to use a web browser to navigate to the website that contains this data.

**Note: **Google's chrome web browser has a nifty built in element inspection function that allows us to quickly analyse html code and 
is recommended for all beginners. 

Using Chrome, navigate to: http://www.data.gov/

Identify at the centre of the webpage which indicates **number of datasets**, right click anywhere on it, and then select **inspect element** from the dropdown menu.

This will cause a window to pop-up on the bottom or side of your screen that displays website's html code. The **number** appears with the word **datasets** and below **GET STARTED**, so scan through the html data until 
we find the line of code that highlights these words on the webpage. 

**NOTE:** Chrome's Inspect Element function is really neat in that it highlights the part of the webpage corresponding to its html code 
allowing you to quickly find what the information we are looking for.

![Figure 1: Snapshot of http://www.data.gov in Chrome Inspect Element window opened](../Inspect_Element1.png)

Locate the line that reads `<small> " Search over "<a href="/metrics">194, 723 datasets</a></small>` and verify it highlights the **number of datasets** at the centre of webpage. This html element contains the 
data we look to get, now lets use Python to extract that data.

**NOTE:** HTML code will vary from website to website but most follow the same structure. Keep in mind as you follow this session material that you may have to enter in different Python script/ code corresponding to the website you're looking to scrape. 
Instead of `<small>`, it could be something else like `<p>`, `<h1>`, `<h3>` or `<h4>` etc.

Open `ipython` or `ipython notebook` or `python` interpreter and type in the following:

~~~{.python}
import urllib
from bs4 import BeautifulSoup
~~~






